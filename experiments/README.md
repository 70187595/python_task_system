# ЭКСПЕРИМЕНТЫ ПО УЛУЧШЕНИЮ НЕЙРОННОЙ СЕТИ

## Описание

В этой папке собраны все эксперименты по улучшению архитектуры и параметров нейронной сети для оценки качества Python кода.

---

## Структура папки

```
experiments/
├── README.md                       # Этот файл
├── hyperparameter_tuning.py        # Скрипт для экспериментов с гиперпараметрами
├── results.json                    # JSON с результатами всех экспериментов
├── RESULTS.md                      # Таблица сравнения всех экспериментов
└── test_results.json               # Результаты тестирования на test set
```

---

## План экспериментов

### Базовая модель (Baseline)
- **Архитектура:** 10 → 8 → 3
- **Learning rate:** 0.01
- **Epochs:** 2000
- **Activation:** Sigmoid
- **Цель:** Установить базовый уровень для сравнения

### Эксперимент 1-3: Размер скрытого слоя
- **Гипотеза:** Больше нейронов = лучшая выразительность
- **Варианты:** hidden_size = [4, 12, 16, 20]
- **Ожидаемый результат:** Оптимум в диапазоне 12-16

### Эксперимент 4-6: Learning Rate
- **Гипотеза:** Оптимальная скорость обучения улучшит сходимость
- **Варианты:** lr = [0.001, 0.005, 0.05, 0.1]
- **Ожидаемый результат:** Оптимум около 0.01

### Эксперимент 7: Количество эпох
- **Гипотеза:** Найти момент начала переобучения
- **Метод:** Отслеживать validation loss
- **Ожидаемый результат:** Оптимум 1500-2500 эпох

### Эксперимент 8: Функции активации
- **Гипотеза:** ReLU может быть эффективнее sigmoid
- **Варианты:** Sigmoid, ReLU, Tanh
- **Ожидаемый результат:** Sigmoid лучше для выхода [0,1]

### Эксперимент 9: Два скрытых слоя
- **Гипотеза:** Глубже = лучше выделение признаков
- **Архитектура:** 10 → 16 → 8 → 3
- **Ожидаемый результат:** Улучшение на 3-5%

### Эксперимент 10: Dropout
- **Гипотеза:** Dropout уменьшит переобучение
- **Варианты:** dropout = [0.2, 0.3, 0.5]
- **Ожидаемый результат:** Лучшая генерализация

---

## Метрики для сравнения

Для каждого эксперимента фиксируются:

1. **Train Loss** - ошибка на обучающей выборке
2. **Validation Loss** - ошибка на валидационной выборке
3. **Test Accuracy** - точность на тестовой выборке
4. **Training Time** - время обучения в секундах
5. **Convergence Speed** - на какой эпохе началась сходимость

---

## Формат результатов (results.json)

```json
{
  "baseline": {
    "name": "Baseline Model",
    "params": {
      "hidden_size": 8,
      "learning_rate": 0.01,
      "epochs": 2000,
      "activation": "sigmoid"
    },
    "metrics": {
      "train_loss": 0.045,
      "val_loss": 0.052,
      "test_accuracy": 0.72,
      "training_time": 125.3
    }
  },
  "exp1_hidden4": {
    "name": "Experiment 1: Hidden Size = 4",
    "params": {
      "hidden_size": 4,
      "learning_rate": 0.01,
      "epochs": 2000,
      "activation": "sigmoid"
    },
    "metrics": {
      "train_loss": 0.068,
      "val_loss": 0.074,
      "test_accuracy": 0.64,
      "training_time": 98.2
    },
    "comparison_to_baseline": {
      "accuracy_diff": -0.08,
      "percent_change": -11.1
    }
  }
}
```

---

## Как запустить эксперименты

### Запуск одного эксперимента:
```bash
python experiments/hyperparameter_tuning.py --experiment exp1 --hidden-size 12
```

### Запуск всех экспериментов:
```bash
python experiments/hyperparameter_tuning.py --run-all
```

### Просмотр результатов:
```bash
python experiments/hyperparameter_tuning.py --show-results
```

---

## Критерии успеха

Эксперимент считается успешным, если:

1. **Accuracy улучшена** на ≥3% относительно baseline
2. **Validation loss** не растёт (нет переобучения)
3. **Training time** не увеличилось более чем в 2 раза

---

## Итоговый выбор параметров

После всех экспериментов будет выбрана **финальная модель** с лучшими параметрами:

- ✅ Лучший hidden_size
- ✅ Лучший learning_rate  
- ✅ Оптимальное количество epochs
- ✅ Лучшая функция активации
- ✅ Нужен ли dropout

---

## Статус выполнения

- [ ] Baseline модель обучена
- [ ] Эксперимент 1: hidden_size=4
- [ ] Эксперимент 2: hidden_size=12
- [ ] Эксперимент 3: hidden_size=16
- [ ] Эксперимент 4: lr=0.001
- [ ] Эксперимент 5: lr=0.05
- [ ] Эксперимент 6: epochs=1000
- [ ] Эксперимент 7: epochs=3000
- [ ] Эксперимент 8: ReLU activation
- [ ] Эксперимент 9: 2 hidden layers
- [ ] Эксперимент 10: dropout
- [ ] Финальная модель выбрана

---

**Этот файл будет обновляться по мере проведения экспериментов.**

