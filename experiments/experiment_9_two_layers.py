"""
–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 9: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –¥–≤—É–º—è —Å–∫—Ä—ã—Ç—ã–º–∏ —Å–ª–æ—è–º–∏
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: 10 ‚Üí 16 ‚Üí 8 ‚Üí 3
–¶–µ–ª—å: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —É–ª—É—á—à–∏—Ç –ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ç–æ—Ä–æ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
"""

import sys
import os
import json
import numpy as np

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.neural_network import SimpleNeuralNetwork


def load_and_prepare_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    with open('data/training_data/training_data.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    X = []
    y = []
    
    for item in data:
        features = item['features']
        target = item['target']
        
        feature_vector = [
            features['lines_of_code'] / 100.0,
            features['functions_count'] / 10.0,
            features['complexity'],
            features['nested_levels'] / 5.0,
            features['variable_names_length'] / 20.0,
            features['comments_ratio'],
            features['imports_count'] / 10.0,
            features['class_count'] / 5.0,
            features['error_handling'],
            features['test_coverage']
        ]
        
        X.append(feature_vector)
        y.append(target)
    
    return np.array(X), np.array(y), len(data)


def main():
    print("üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 9: –î–≤–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è (10‚Üí16‚Üí8‚Üí3)")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X, y, data_size = load_and_prepare_data()
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {data_size} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç—å —Å –¥–≤—É–º—è —Å–∫—Ä—ã—Ç—ã–º–∏ —Å–ª–æ—è–º–∏
    # –ù–ï –∑–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å –Ω—É–ª—è
    network = SimpleNeuralNetwork(
        input_size=10,
        hidden_size=16,      # –ü–µ—Ä–≤—ã–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
        output_size=3,
        activation='sigmoid',
        hidden_size2=8       # –í—Ç–æ—Ä–æ–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
    )
    
    # –ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å–∞, —á—Ç–æ–±—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    if network.use_two_hidden_layers:
        network.weights_input_hidden1 = np.random.randn(10, 16) * 0.1
        network.weights_hidden1_hidden2 = np.random.randn(16, 8) * 0.1
        network.weights_hidden2_output = np.random.randn(8, 3) * 0.1
        network.bias_hidden1 = np.zeros((1, 16))
        network.bias_hidden2 = np.zeros((1, 8))
        network.bias_output = np.zeros((1, 3))
    
    network.learning_rate = 0.01
    
    print(f"\nüîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: 10 ‚Üí 16 ‚Üí 8 ‚Üí 3")
    print(f"   Learning rate: 0.01")
    print(f"   Epochs: 2000")
    print(f"   Activation: Sigmoid")
    print(f"   –¢–∏–ø: –î–≤—É—Ö—Å–ª–æ–π–Ω–∞—è –≥–ª—É–±–æ–∫–∞—è —Å–µ—Ç—å")
    
    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
    params_w1 = 10 * 16
    params_b1 = 16
    params_w2 = 16 * 8
    params_b2 = 8
    params_w3 = 8 * 3
    params_b3 = 3
    total_params = params_w1 + params_b1 + params_w2 + params_b2 + params_w3 + params_b3
    
    print(f"\nüìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
    print(f"   W1 (10√ó16): {params_w1}")
    print(f"   b1: {params_b1}")
    print(f"   W2 (16√ó8): {params_w2}")
    print(f"   b2: {params_b2}")
    print(f"   W3 (8√ó3): {params_w3}")
    print(f"   b3: {params_b3}")
    print(f"   –í—Å–µ–≥–æ: {total_params} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    print(f"   –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å baseline (115 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤): +{total_params - 115} (+{((total_params - 115)/115)*100:.1f}%)")
    
    # –û–±—É—á–∞–µ–º
    training_data = [(X[i:i+1], y[i:i+1]) for i in range(len(X))]
    print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    
    history = network.train(training_data, epochs=2000)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    os.makedirs('experiments/results', exist_ok=True)
    network.save_model('experiments/results/model_exp9_two_layers.json')
    
    with open('experiments/results/history_exp9_two_layers.json', 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=2)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"   –ù–∞—á–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {history['loss'][0]:.4f}")
    print(f"   –ö–æ–Ω–µ—á–Ω–∞—è –æ—à–∏–±–∫–∞: {history['loss'][-1]:.4f}")
    improvement = (1 - history['loss'][-1]/history['loss'][0])*100 if history['loss'][0] > 0 else 0
    print(f"   –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.1f}%")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
    print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö:")
    test_indices = [0, 50, 100, 150, 200]
    total_error = 0
    test_count = 0
    
    for idx in test_indices:
        if idx < len(X):
            prediction = network.predict(X[idx]).flatten()
            target = y[idx]
            error = np.mean(np.abs(prediction - target))
            total_error += error
            test_count += 1
            
            print(f"\n   –ü—Ä–∏–º–µ—Ä {idx}:")
            print(f"   –û–∂–∏–¥–∞–µ–º–æ–µ:    [{target[0]:.2f}, {target[1]:.2f}, {target[2]:.2f}]")
            print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: [{prediction[0]:.2f}, {prediction[1]:.2f}, {prediction[2]:.2f}]")
            print(f"   –û—à–∏–±–∫–∞: {error:.4f}")
    
    avg_test_error = total_error / test_count if test_count > 0 else 0
    print(f"\n   üìä –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–∞—Ö: {avg_test_error:.4f}")
    
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: experiments/results/model_exp9_two_layers.json")
    print(f"‚úÖ –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: experiments/results/history_exp9_two_layers.json")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å baseline
    print(f"\nüìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏:")
    print(f"   Baseline (8 –Ω–µ–π—Ä–æ–Ω–æ–≤, 1 —Å–ª–æ–π):     0.0056")
    print(f"   –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 5 (lr=0.05):           0.0038 üèÜ")
    print(f"   –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 8 (ReLU):              0.0047")
    print(f"   –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 9 (2 —Å–ª–æ—è):            {history['loss'][-1]:.4f}")
    
    baseline_error = 0.0056
    if history['loss'][-1] < baseline_error:
        improvement_vs_baseline = ((baseline_error - history['loss'][-1])/baseline_error)*100
        print(f"   ‚úÖ –î–≤–∞ —Å–ª–æ—è –ª—É—á—à–µ baseline –Ω–∞ {improvement_vs_baseline:.1f}%")
    elif history['loss'][-1] > baseline_error:
        degradation = ((history['loss'][-1] - baseline_error)/baseline_error)*100
        print(f"   ‚ö†Ô∏è  –î–≤–∞ —Å–ª–æ—è —Ö—É–∂–µ baseline –Ω–∞ {degradation:.1f}%")
    else:
        print(f"   üîÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã —Å baseline")
    
    print(f"\nüí° –í—ã–≤–æ–¥—ã:")
    if history['loss'][-1] < 0.0045:
        print(f"   ‚úÖ –ì–ª—É–±–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–∫–∞–∑–∞–ª–∞ –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã!")
        print(f"   ‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª–æ–π –ø–æ–º–æ–≥ —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ")
    elif history['loss'][-1] < 0.0056:
        print(f"   ‚úÖ –ì–ª—É–±–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ")
        print(f"   ‚ö†Ô∏è  –ù–æ –≤—ã–∏–≥—Ä—ã—à –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    else:
        print(f"   ‚ö†Ô∏è  –ì–ª—É–±–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ –ø–æ–∫–∞–∑–∞–ª–∞ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞")
        print(f"   üí° –í–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è {total_params} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")


if __name__ == '__main__':
    main()

