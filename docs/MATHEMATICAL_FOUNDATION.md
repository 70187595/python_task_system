# МАТЕМАТИЧЕСКОЕ ОБОСНОВАНИЕ АРХИТЕКТУРЫ НЕЙРОННОЙ СЕТИ

## Введение

Данный документ содержит полное математическое описание нейронной сети для оценки качества Python кода, реализованной в файле `app/models/neural_network.py`.

---

## 1. АРХИТЕКТУРА НЕЙРОННОЙ СЕТИ

### 1.1 Общая структура

Нейронная сеть представляет собой **многослойный персептрон (MLP)** с одним скрытым слоем:

```
Входной слой (10) → Скрытый слой (8) → Выходной слой (3)
```

### 1.2 Математическое описание слоёв

#### Входной слой

**Вектор входных признаков:**

```
x = [x₁, x₂, x₃, x₄, x₅, x₆, x₇, x₈, x₉, x₁₀]ᵀ ∈ ℝ¹⁰
```

где:
- x₁ = lines_of_code / 100
- x₂ = functions_count / 10
- x₃ = complexity / 10
- x₄ = nested_levels / 5
- x₅ = variable_names_length / 20
- x₆ = comments_ratio ∈ [0, 1]
- x₇ = imports_count / 10
- x₈ = class_count / 5
- x₉ = error_handling ∈ {0, 1}
- x₁₀ = test_coverage ∈ [0, 1]

**Размерность:** x ∈ ℝ¹⁰

---

#### Скрытый слой

**Линейное преобразование:**

```
z⁽¹⁾ = W⁽¹⁾x + b⁽¹⁾
```

где:
- W⁽¹⁾ ∈ ℝ⁸ˣ¹⁰ — матрица весов входного слоя
- b⁽¹⁾ ∈ ℝ⁸ — вектор смещений (bias)
- z⁽¹⁾ ∈ ℝ⁸ — взвешенная сумма входов

**Функция активации (sigmoid):**

```
h = σ(z⁽¹⁾)

где σ(z) = 1 / (1 + e⁻ᶻ)
```

**Размерность:** h ∈ ℝ⁸

**Связь с кодом:** см. `neural_network.py`, строки 62-63

```python
# Строка 62: вычисление z⁽¹⁾ = W⁽¹⁾x + b⁽¹⁾
hidden_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden

# Строка 63: применение σ(z⁽¹⁾)
hidden_output = self.sigmoid(hidden_input)
```

---

#### Выходной слой

**Линейное преобразование:**

```
z⁽²⁾ = W⁽²⁾h + b⁽²⁾
```

где:
- W⁽²⁾ ∈ ℝ³ˣ⁸ — матрица весов скрытого слоя
- b⁽²⁾ ∈ ℝ³ — вектор смещений
- z⁽²⁾ ∈ ℝ³ — взвешенная сумма скрытого слоя

**Функция активации:**

```
y = σ(z⁽²⁾)
```

**Размерность:** y ∈ ℝ³

**Интерпретация выхода:**

```
y = [y₁, y₂, y₃]ᵀ

где:
y₁ ∈ [0, 1] — оценка правильности (correctness)
y₂ ∈ [0, 1] — оценка эффективности (efficiency)
y₃ ∈ [0, 1] — оценка читаемости (readability)
```

**Связь с кодом:** см. `neural_network.py`, строки 66-67

```python
# Строка 66: вычисление z⁽²⁾ = W⁽²⁾h + b⁽²⁾
output_input = np.dot(hidden_output, self.weights_hidden_output) + self.bias_output

# Строка 67: применение σ(z⁽²⁾)
output = self.sigmoid(output_input)
```

---

## 2. ПРЯМОЕ РАСПРОСТРАНЕНИЕ (Forward Pass)

### 2.1 Полный алгоритм

**Шаг 1:** Вычисление скрытого слоя

```
z⁽¹⁾ = W⁽¹⁾x + b⁽¹⁾
h = σ(z⁽¹⁾)
```

**Шаг 2:** Вычисление выходного слоя

```
z⁽²⁾ = W⁽²⁾h + b⁽²⁾
y = σ(z⁽²⁾)
```

### 2.2 Матричная форма

Для батча из N примеров:

```
X ∈ ℝᴺˣ¹⁰ — матрица входов
H = σ(XW⁽¹⁾ᵀ + B⁽¹⁾) ∈ ℝᴺˣ⁸
Y = σ(HW⁽²⁾ᵀ + B⁽²⁾) ∈ ℝᴺˣ³
```

где B⁽¹⁾, B⁽²⁾ — расширенные векторы смещений (broadcasting).

**Связь с кодом:** см. `neural_network.py`, метод `forward()`, строки 51-69

---

## 3. ФУНКЦИЯ АКТИВАЦИИ SIGMOID

### 3.1 Определение

```
σ(z) = 1 / (1 + e⁻ᶻ)
```

### 3.2 Свойства

1. **Диапазон:** σ(z) ∈ (0, 1)
2. **Монотонность:** σ'(z) > 0 для всех z
3. **Симметрия:** σ(-z) = 1 - σ(z)
4. **Предел:** 
   - lim(z→+∞) σ(z) = 1
   - lim(z→-∞) σ(z) = 0

### 3.3 Производная

```
σ'(z) = σ(z) · (1 - σ(z))
```

**Упрощение для уже вычисленных значений:**

Если a = σ(z), то:

```
σ'(z) = a · (1 - a)
```

**Связь с кодом:** см. `neural_network.py`, строки 43-49

```python
def sigmoid(self, x):
    """Функция активации sigmoid"""
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def sigmoid_derivative(self, x):
    """Производная функции sigmoid"""
    return x * (1 - x)  # x уже содержит σ(z)
```

---

## 4. ФУНКЦИЯ ПОТЕРЬ

### 4.1 Mean Squared Error (MSE)

Для одного примера:

```
L(y, ŷ) = 1/2 ||y - ŷ||² = 1/2 Σᵢ₌₁³ (yᵢ - ŷᵢ)²
```

где:
- y = [y₁, y₂, y₃]ᵀ — целевые значения (target)
- ŷ = [ŷ₁, ŷ₂, ŷ₃]ᵀ — предсказания сети

Для батча из N примеров:

```
L = 1/N Σⱼ₌₁ᴺ Σᵢ₌₁³ (yⱼᵢ - ŷⱼᵢ)²
```

### 4.2 Обоснование выбора MSE

**Почему MSE?**

1. **Задача регрессии:** Мы предсказываем непрерывные значения в диапазоне [0, 1]
2. **Дифференцируемость:** MSE легко дифференцируется
3. **Штраф за большие ошибки:** Квадрат ошибки увеличивает штраф
4. **Простота вычислений:** Градиенты вычисляются просто

**Связь с кодом:** см. `neural_network.py`, строка 117

```python
# Строка 117: расчет ошибки
error = np.mean(np.square(target - output))
```

---

## 5. ОБРАТНОЕ РАСПРОСТРАНЕНИЕ ОШИБКИ (Backpropagation)

### 5.1 Градиенты для выходного слоя

**Ошибка выходного слоя:**

```
E_out = y - ŷ
```

**Градиент по z⁽²⁾:**

```
δ⁽²⁾ = E_out ⊙ σ'(z⁽²⁾) = E_out ⊙ ŷ ⊙ (1 - ŷ)
```

где ⊙ — поэлементное умножение (Hadamard product)

**Градиенты параметров:**

```
∂L/∂W⁽²⁾ = hᵀδ⁽²⁾

∂L/∂b⁽²⁾ = δ⁽²⁾
```

### 5.2 Градиенты для скрытого слоя

**Ошибка скрытого слоя:**

```
E_hidden = (W⁽²⁾)ᵀδ⁽²⁾
```

**Градиент по z⁽¹⁾:**

```
δ⁽¹⁾ = E_hidden ⊙ σ'(z⁽¹⁾) = E_hidden ⊙ h ⊙ (1 - h)
```

**Градиенты параметров:**

```
∂L/∂W⁽¹⁾ = xᵀδ⁽¹⁾

∂L/∂b⁽¹⁾ = δ⁽¹⁾
```

### 5.3 Связь с кодом

См. `neural_network.py`, метод `backward()`, строки 71-96

```python
# Строка 83: ошибка выходного слоя E_out = target - output
output_error = target - output

# Строка 84: градиент δ⁽²⁾
output_delta = output_error * self.sigmoid_derivative(output)

# Строка 87: ошибка скрытого слоя E_hidden
hidden_error = np.dot(output_delta, self.weights_hidden_output.T)

# Строка 88: градиент δ⁽¹⁾
hidden_delta = hidden_error * self.sigmoid_derivative(hidden_output)

# Строка 91: обновление W⁽²⁾
self.weights_hidden_output += np.dot(hidden_output.T, output_delta) * self.learning_rate

# Строка 92: обновление W⁽¹⁾
self.weights_input_hidden += np.dot(inputs.T, hidden_delta) * self.learning_rate
```

---

## 6. ГРАДИЕНТНЫЙ СПУСК

### 6.1 Правило обновления весов

```
W := W + α · ∂L/∂W
b := b + α · ∂L/∂b
```

где α — learning rate (скорость обучения)

**Примечание:** Знак "+" вместо "-", потому что ошибка вычисляется как `target - output`, а не `output - target`.

### 6.2 Параметры обучения

- **Learning rate (α):** 0.01 (по умолчанию)
- **Количество эпох:** 2000 (по умолчанию)
- **Размер батча:** 1 (онлайн обучение, каждый пример отдельно)

**Связь с кодом:** см. `neural_network.py`, строка 38

```python
self.learning_rate = 0.01
```

---

## 7. ИНИЦИАЛИЗАЦИЯ ВЕСОВ

### 7.1 Случайная инициализация

Веса инициализируются случайными значениями из нормального распределения:

```
W⁽¹⁾ ~ N(0, 0.01) ∈ ℝ⁸ˣ¹⁰
W⁽²⁾ ~ N(0, 0.01) ∈ ℝ³ˣ⁸

b⁽¹⁾ = 0 ∈ ℝ⁸
b⁽²⁾ = 0 ∈ ℝ³
```

### 7.2 Обоснование

1. **Малые значения:** 0.01 предотвращает насыщение sigmoid
2. **Случайность:** Разрушает симметрию, позволяет нейронам обучаться разному
3. **Нулевые смещения:** Стандартная практика

**Связь с кодом:** см. `neural_network.py`, строки 29-35

```python
# Строки 30-31: инициализация весов
self.weights_input_hidden = np.random.randn(input_size, hidden_size) * 0.1
self.weights_hidden_output = np.random.randn(hidden_size, output_size) * 0.1

# Строки 34-35: инициализация смещений
self.bias_hidden = np.zeros((1, hidden_size))
self.bias_output = np.zeros((1, output_size))
```

---

## 8. ОБОСНОВАНИЕ АРХИТЕКТУРЫ

### 8.1 Почему 10 входов?

Выбранные 10 признаков покрывают основные аспекты качества кода:
- **Размер:** lines_of_code
- **Структура:** functions_count, class_count, nested_levels
- **Сложность:** complexity
- **Стиль:** variable_names_length, comments_ratio
- **Практики:** imports_count, error_handling, test_coverage

Эти признаки легко извлекаются из AST и достаточны для базовой оценки.

### 8.2 Почему 8 скрытых нейронов?

Эмпирическое правило:

```
h = (n_input + n_output) / 2 = (10 + 3) / 2 ≈ 6-8
```

8 нейронов обеспечивают:
- Достаточную выразительность для выделения закономерностей
- Отсутствие переобучения (меньше параметров)
- Быструю сходимость

**Всего параметров:**
```
W⁽¹⁾: 10 × 8 = 80
b⁽¹⁾: 8
W⁽²⁾: 8 × 3 = 24
b⁽²⁾: 3
────────────────
Итого: 115 параметров
```

### 8.3 Почему 3 выхода?

Три независимые оценки качества:
1. **Correctness** — правильность работы алгоритма
2. **Efficiency** — эффективность (сложность, оптимальность)
3. **Readability** — читаемость и стиль

Это три разных аспекта, которые могут быть независимыми:
- Код может быть правильным, но медленным
- Код может быть эффективным, но нечитаемым
- И т.д.

### 8.4 Почему sigmoid?

**Преимущества sigmoid:**
1. **Выход в [0, 1]** — идеально для процентов качества
2. **Гладкость** — дифференцируема везде
3. **Нелинейность** — может выделять сложные зависимости

**Недостатки:**
- Проблема затухающих градиентов (vanishing gradients)
- Для глубоких сетей лучше ReLU

**Для нашей задачи:** Sigmoid подходит, т.к. сеть мелкая (1 скрытый слой).

---

## 9. ПРИМЕНИМОСТЬ К ЗАДАЧЕ ОЦЕНКИ КАЧЕСТВА КОДА

### 9.1 Почему нейронная сеть?

**Альтернативы:**
1. **Эвристические правила:** "Если complexity > 10, то плохо"
   - Минус: негибкие, не учитывают контекст
   
2. **Линейная регрессия:** y = w₁x₁ + ... + w₁₀x₁₀
   - Минус: не может выделить нелинейные зависимости

**Нейронная сеть:**
- Может выделять **нелинейные зависимости** между признаками
- Например: "Высокая сложность + хорошие комментарии = приемлемо"
- **Обучается** на примерах, а не программируется вручную

### 9.2 Задача: регрессия или классификация?

**Регрессия:** Предсказываем непрерывные значения в [0, 1]

Почему не классификация ("отлично/хорошо/плохо")?
- Потеря информации (0.85 и 0.75 — оба "хорошо", но разница есть)
- Регрессия гибче и информативнее

### 9.3 Ограничения подхода

1. **Малый датасет:** 20 примеров → риск переобучения
2. **Простая архитектура:** 1 скрытый слой → ограниченная выразительность
3. **Субъективность:** Оценка качества зависит от эксперта
4. **Отсутствие контекста:** Не анализируем семантику кода, только метрики

---

## 10. СРАВНЕНИЕ С ДРУГИМИ АРХИТЕКТУРАМИ

### 10.1 Почему не CNN (свёрточная сеть)?

CNN хороши для:
- Изображений (пространственные паттерны)
- Текста (локальные паттерны в последовательности)

Наша задача:
- **Вектор признаков**, а не последовательность
- Признаки независимы (порядок не важен)

### 10.2 Почему не RNN/LSTM?

RNN/LSTM хороши для:
- Последовательностей (текст, временные ряды)
- Когда важен порядок элементов

Наша задача:
- **Фиксированный вектор** из 10 чисел
- Порядок признаков не важен

### 10.3 Почему не глубокая сеть (3+ слоя)?

Глубокие сети нужны для:
- Сложных нелинейных зависимостей
- Больших датасетов (тысячи примеров)

Наша задача:
- **Простые зависимости** между метриками кода
- **Малый датасет** (20-500 примеров)
- Риск **переобучения** с большим количеством слоёв

**Вывод:** Простой MLP с 1 скрытым слоем — оптимальный выбор.

---

## 11. ИТОГОВАЯ АРХИТЕКТУРА

```
┌──────────────────┐
│  Входной слой    │
│   x ∈ ℝ¹⁰        │
└────────┬─────────┘
         │ W⁽¹⁾ ∈ ℝ⁸ˣ¹⁰
         │ b⁽¹⁾ ∈ ℝ⁸
         ↓
┌──────────────────┐
│  Скрытый слой    │
│   h = σ(Wx + b)  │
│   h ∈ ℝ⁸         │
└────────┬─────────┘
         │ W⁽²⁾ ∈ ℝ³ˣ⁸
         │ b⁽²⁾ ∈ ℝ³
         ↓
┌──────────────────┐
│  Выходной слой   │
│   y = σ(Wh + b)  │
│   y ∈ ℝ³         │
└──────────────────┘
```

**Параметры:**
- Всего: 115 параметров
- Learning rate: 0.01
- Функция активации: Sigmoid
- Функция потерь: MSE
- Оптимизатор: Градиентный спуск

---

**Этот документ будет обновляться по мере эволюции архитектуры.**

